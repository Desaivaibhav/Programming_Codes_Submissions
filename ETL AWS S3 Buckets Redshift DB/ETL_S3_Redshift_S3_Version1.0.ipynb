{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift Connection Successful!\n",
      "Source S3 files being retrieved\n",
      "20170701_20170701165514569.gz\n",
      "20170701\n",
      "s3://tmp2.sl.com/scanbuy_VD-20170701\n",
      "Unloaded rows to S3 Destination Bucket\n",
      "144923\n",
      "20170701_20170702004210139.gz\n",
      "20170702\n",
      "s3://tmp2.sl.com/scanbuy_VD-20170702\n",
      "Unloaded rows to S3 Destination Bucket\n",
      "156539\n",
      "20170701_20170702013337800.gz\n",
      "20170702\n",
      "s3://tmp2.sl.com/scanbuy_VD-20170702\n",
      "Unloaded rows to S3 Destination Bucket\n",
      "39503\n",
      "20170701_20170702154430430.gz\n",
      "20170702\n",
      "s3://tmp2.sl.com/scanbuy_VD-20170702\n",
      "Unloaded rows to S3 Destination Bucket\n",
      "258260\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import psycopg2\n",
    "import boto\n",
    "import boto.s3.connection\n",
    "import json\n",
    "import sys\n",
    "\n",
    "#Import and parse credentials_File\n",
    "input_data='credentials.json'\n",
    "\n",
    "cred_input = json.loads(open(input_data).read())\n",
    "\n",
    "\n",
    "#variable assignments\n",
    "#redshift credentials\n",
    "database=cred_input['Redshift_credentials']['database']\n",
    "RSURL=cred_input['Redshift_credentials']['RSURL']\n",
    "hostname=cred_input['Redshift_credentials']['hostname']\n",
    "dbuser=cred_input['Redshift_credentials']['dbuser']\n",
    "RSpwd=cred_input['Redshift_credentials']['pwd']\n",
    "redshift_port=cred_input['Redshift_credentials']['redshift_port']\n",
    "RSSchema=cred_input['Redshift_credentials']['RSSchema']\n",
    "\n",
    "#S3 credentials\n",
    "S3accesskey = cred_input['S3_credentials']['S3accesskey']\n",
    "S3secretkey = cred_input['S3_credentials']['S3secretkey']\n",
    "S3sourcebucket=cred_input['S3_credentials']['S3sourcebucket']\n",
    "S3destbucket=cred_input['S3_credentials']['S3destbucket']\n",
    "S3bucketloc=cred_input['S3_credentials']['S3bucketloc']\n",
    "\n",
    "\n",
    "#defining redshift destination table as part of ETL if it doesnt exists\n",
    "#########################################\n",
    "try:\n",
    "    con = psycopg2.connect(dbname= database, host=hostname, \n",
    "port= redshift_port, user= dbuser, password= RSpwd)\n",
    "    print(\"Redshift Connection Successful!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sys.exit(\"Unable to connect to Redshift\");\n",
    "\n",
    "    \n",
    "cursor = con.cursor()\n",
    "sql=\"CREATE TABLE IF NOT EXISTS buy_vd(  PID  INTEGER NOT NULL,  Ad_id        VARCHAR(50) NOT NULL,  id_type      VARCHAR(20) NOT NULL,  UA       TEXT NOT NULL,  IP   VARCHAR(20) NOT NULL,  epoch_timestamp   TIMESTAMP NOT NULL,  lat DECIMAL(8, 5) NOT NULL,  long  DECIMAL(8, 5) NOT NULL,  accuracy INTEGER  NOT NULL,  optout VARCHAR(1) NOT NULL,  country VARCHAR(20) NOT NULL,  proc_date     VARCHAR(10) NOT NULL);\"\n",
    "try:\n",
    "    cursor.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    sys.exit(\"Unable to create table in Redshift\");\n",
    "\n",
    "#destination:Redshift table name\n",
    "desttable='buy_vd'\n",
    "#########################################\n",
    "\n",
    "\n",
    "\n",
    "#download files from S3 as list \n",
    "#########################################\n",
    "def download_sourcefile_list_S3(S3key,S3secretkey,src_bucket):\n",
    "    file_list=[]\n",
    "    # since bucket name contains dot issue accessing it ref: https://github.com/boto/boto/issues/2836 \n",
    "    #using calling_format=boto.s3.connection.OrdinaryCallingFormat()\n",
    "    conn = boto.connect_s3(\n",
    "        aws_access_key_id = S3key,\n",
    "        aws_secret_access_key = S3secretkey,\n",
    "        calling_format=boto.s3.connection.OrdinaryCallingFormat()\n",
    "        )\n",
    "    bucket = conn.get_bucket(src_bucket)\n",
    "    \n",
    "    try:\n",
    "        for key in bucket.list():\n",
    "            file_list.append(key.name)\n",
    "        print(\"Source S3 files being retrieved\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to access S3 source files\");\n",
    "        \n",
    "    #List of files thouse are at source bucket\n",
    "    return file_list\n",
    "\n",
    "#########################################\n",
    "\n",
    "#Create Staging table\n",
    "#Copy the file to Redshift Stage Table\n",
    "#Query the staged data to get year of file\n",
    "#Get(unload) required data(format) from stage table and export to destination S3 bucket\n",
    "#get the unloaded count\n",
    "#Load the data from staging table to destination Table with conversion of epoch time to Timestamp\n",
    "#Drop the staging table\n",
    "\n",
    "#########################################\n",
    "def copy_to_redshift_cluster_and_export_to_S3(db,hst,prt,usr,pwd,desttable,S3path,S3key,S3secretkey,S3bucketloc,delim,quote,zipsol,dest_path):\n",
    "    con=psycopg2.connect(dbname= db, host=hst, \n",
    "    port= prt, user= usr, password= RSpwd)\n",
    "    cursor = con.cursor()\n",
    "    \n",
    "    #create staging table\n",
    "    sql=\"CREATE TABLE IF NOT EXISTS buy_vd_stage(  PID  INTEGER NOT NULL,  Ad_id        VARCHAR(50) NOT NULL,  id_type      VARCHAR(20) NOT NULL,  UA       TEXT NOT NULL,  IP   VARCHAR(20) NOT NULL,  epoch_timestamp   BIGINT NOT NULL,  lat DECIMAL(8, 5) NOT NULL,  long  DECIMAL(8, 5) NOT NULL,  accuracy INTEGER  NOT NULL,  optout VARCHAR(1) NOT NULL,  country VARCHAR(20) NOT NULL,  proc_date     VARCHAR(10) NOT NULL);\"\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        con.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit('Staging table creation failed')\n",
    "        \n",
    "    \n",
    "    Rstagstable='buy_vd_stage'\n",
    "    \n",
    "    #copy to redshift table\n",
    "    cpy_cmd=\"\"\"COPY %s FROM '%s'  \\\n",
    "      credentials 'aws_access_key_id=%s;aws_secret_access_key=%s' \\\n",
    "       region '%s' delimiter '%s' %s %s;\"\"\" %\\\n",
    "      (Rstagstable, S3path, S3key, S3secretkey,S3bucketloc,delim,quote,zipsol)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(cpy_cmd)\n",
    "        con.commit\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to copy file data to redshift cliuster(table)\");\n",
    "    \n",
    "    #get year\n",
    "    gtyrsql=\"SELECT REPLACE(proc_date,'-','') AS YR FROM %s LIMIT 1;\"%\\\n",
    "             (Rstagstable)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(gtyrsql)\n",
    "        rows = cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to query staging table\");\n",
    "    \n",
    "    YR=''\n",
    "    for row in rows:\n",
    "        YR=row[0]\n",
    "    print(YR)\n",
    "    \n",
    "    dest_path=dest_path+YR\n",
    "    print(dest_path)\n",
    "    \n",
    "    \n",
    "        \n",
    "    #Unloading to S3 destinatio bucket\n",
    "    #print('unloading')\n",
    "    unld_cmd=\"\"\"UNLOAD ('SELECT DISTINCT CHR(123)||  \\''\"Ad_id\":\"\\''||nvl(Ad_id,\\''\\'')||\\''\",\\''||  \\''\"id_type\":\\''||nvl(id_type)||\\''\",\\''||  \\''\"lat\":\\''||nvl(CAST(round(lat,3)AS FLOAT))||\\''\",\\''||  \\''\"long\":\\''||nvl(CAST(round(long,3)AS FLOAT))|| chr(125) FROM %s;') TO '%s'  \\\n",
    "        credentials 'aws_access_key_id=%s;aws_secret_access_key=%s' \\\n",
    "        GZIP ALLOWOVERWRITE   parallel off;\"\"\" \\\n",
    "        % (Rstagstable,dest_path,S3key,S3secretkey)\n",
    "       \n",
    "    try:\n",
    "        cursor.execute(unld_cmd)\n",
    "        con.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to unload redshift table data to S3 bucket\");\n",
    "    \n",
    "    #get count of rows unloaded\n",
    "    rowsql=\"select pg_last_unload_count();\"\n",
    "    try:\n",
    "        cursor.execute(rowsql)\n",
    "        rows = cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to get unloaded data rowcount\");\n",
    "    \n",
    "    for row in rows:\n",
    "        rowcount=row[0]\n",
    "        \n",
    "    print('Unloaded rows to S3 Destination Bucket')\n",
    "    print(rowcount)\n",
    "    \n",
    "    #sqluniq_adid=\"SELECT DISTINCT Ad_id FROM buyETL_VaiDes;\"\n",
    "    #cursor.execute(sqluniq_adid)\n",
    "    #rows = cursor.fetchall()\n",
    "    \n",
    "    Unique_Ad_id=[]\n",
    "    #for row in rows:\n",
    "     #   Unique_Ad_id.append(row[0])\n",
    "    \n",
    "   # print('unique')\n",
    "    #print(Unique_Ad_id)\n",
    "    \n",
    "    #loadng data into final table\n",
    "    #print('finaltable loading')\n",
    "    finaltableload=\"INSERT INTO %s (PID, Ad_id, id_type, UA, IP, epoch_timestamp, lat, long, accuracy, optout, country, proc_date) \\\n",
    "                    SELECT PID, Ad_id, id_type, UA, IP, (TIMESTAMP 'epoch' + epoch_timestamp * INTERVAL '1 Second ')  AS val_timestamp,\\\n",
    "                    lat, long, accuracy, optout, country, proc_date FROM %s ;\"%\\\n",
    "                        (desttable,Rstagstable)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(finaltableload)\n",
    "        con.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Final table loading form stage table failed\");\n",
    "    \n",
    "    #drop staging table\n",
    "    drpcmd=\"DROP TABLE %s;\"%\\\n",
    "             (Rstagstable)\n",
    "        \n",
    "    try:\n",
    "        cursor.execute(drpcmd)\n",
    "        con.commit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(\"Unable to drop staging table\");\n",
    "        \n",
    "    #print('dropped')\n",
    "    \n",
    "    #close the connection\n",
    "    con.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#delim=','\n",
    "#quote='removequotes'\n",
    "#zipsol='GZIP'   \n",
    "#Get the list of S3:source bucket  files     \n",
    "SrcS3files=download_sourcefile_list_S3(S3accesskey,S3secretkey,S3sourcebucket)\n",
    "#print(SrcS3files)\n",
    "#Load each file and export the required data to desyination S3 bucket\n",
    "for file in SrcS3files:\n",
    "    print(file)\n",
    "    copy_to_redshift_cluster_and_export_to_S3(database,hostname,redshift_port,dbuser,RSpwd,desttable,'s3://'+S3sourcebucket+'/'+file,S3accesskey,S3secretkey,S3bucketloc,',','removequotes','GZIP','s3://'+S3destbucket+'/'+'buy_VD-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://tmp1.sl.com/20170701_20170701165514569.gz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://tmp2.sl.com/scanbuy-201707011045'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
